# src/process_all_data.py

from pathlib import Path
import cv2
import random
import csv
from tqdm import tqdm
import os
import sys

# --- ê²½ë¡œ ì„¤ì • ---
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

from .paths import DATA_DIR

# ğŸš¨ [Worker ëª¨ë“ˆ ì„í¬íŠ¸]
# ì–¼êµ´ í¬ë¡­(2ë‹¨ê³„)ê³¼ ì¸ë±ì‹±(3ë‹¨ê³„)ì€ ì™¸ë¶€ ëª¨ë“ˆì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
try:
    from .extract_faces import run_face_extraction as run_face_extraction_runner
    from .build_index import run_indexing as run_indexing_runner
except ImportError as e:
    print(f"[ERROR] Failed to import worker modules: {e}")
    print("Check if 'extract_faces.py' and 'build_index.py' exist in src/ folder.")
    sys.exit(1)


# ìƒìˆ˜ ì •ì˜
FRAME_PER_VIDEO = 5
VIDEO_LIMIT_PER_DATASET = 0
random.seed(42)


# ============================================
# I. ë°ì´í„°ì…‹ ì •ì˜ (Manager ì—­í• )
# ============================================

DATASETS = [
    {
        "name": "FaceForensics++ (Training)",
        "input_root": DATA_DIR / "raw" / "ffpp_c23" / "FaceForensics++_C23",
        "frame_output_dir": DATA_DIR / "interim" / "frames_ffpp",
        "frame_csv": DATA_DIR / "processed" / "indices" / "frames_ffpp.csv",
        "face_output_dir": DATA_DIR / "processed" / "faces_ffpp",
        "face_csv": DATA_DIR / "processed" / "indices" / "faces_ffpp.csv",
        "index_prefix": DATA_DIR / "processed" / "indices" / "ffpp",
    },
    {
        "name": "Celeb-DF v2 (Cross-Eval)",
        "input_root": DATA_DIR / "raw" / "Celeb_DF" / "celebdf_v2",
        "frame_output_dir": DATA_DIR / "interim" / "frames_celebdf",
        "frame_csv": DATA_DIR / "processed" / "indices" / "frames_celebdf.csv",
        "face_output_dir": DATA_DIR / "processed" / "faces_celebdf",
        "face_csv": DATA_DIR / "processed" / "indices" / "faces_celebdf.csv",
        "index_prefix": DATA_DIR / "processed" / "indices" / "celebdf",
    },
]


# ============================================
# II. 1ë‹¨ê³„: í”„ë ˆì„ ì¶”ì¶œ ë¡œì§ (Worker ì—­í• )
# ============================================


def sample_indices(num_frames: int, k: int):
    """ì „ì²´ í”„ë ˆì„ ì¤‘ kê°œë¥¼ ê· ì¼í•˜ê²Œ ìƒ˜í”Œë§."""
    if num_frames <= k:
        return list(range(num_frames))
    return sorted(random.sample(range(num_frames), k))


def extract_frames_from_video(video_path: Path, out_dir: Path):
    """ë‹¨ì¼ ì˜ìƒ íŒŒì¼ì—ì„œ ì¼ë¶€ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ì´ë¯¸ì§€ë¡œ ì €ì¥."""
    cap = cv2.VideoCapture(str(video_path))
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if total == 0:
        cap.release()
        return []

    indices = sample_indices(total, FRAME_PER_VIDEO)
    saved_paths = []

    # ì¹´í…Œê³ ë¦¬ëª… ì¶”ì¶œ (ì˜ˆ: Celeb-real, Deepfakes)
    category_name = video_path.parent.name

    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ok, frame = cap.read()
        if not ok:
            continue

        filename = f"{category_name}_{video_path.stem}_f{idx:05d}.jpg"
        out_path = out_dir / filename

        cv2.imwrite(str(out_path), frame)
        saved_paths.append(out_path)

    cap.release()
    return saved_paths


def run_extraction_pipeline(dataset):
    """
    í”„ë ˆì„ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ (Real/Fake ë°¸ëŸ°ì‹± ë¡œì§ í¬í•¨)
    """
    RAW_DATA_ROOT = dataset["input_root"]
    FRAMES_DIR = dataset["frame_output_dir"]
    INDEX_CSV = dataset["frame_csv"]

    FRAMES_DIR.mkdir(parents=True, exist_ok=True)
    INDEX_CSV.parent.mkdir(parents=True, exist_ok=True)

    print(f"[DEBUG] Searching in absolute path: {RAW_DATA_ROOT.resolve()}")

    # ëª¨ë“  mp4 íŒŒì¼ ì°¾ê¸°
    all_videos = list(RAW_DATA_ROOT.rglob("*.mp4"))

    if not all_videos:
        print(f"[ERROR] No MP4 files found in {RAW_DATA_ROOT}. Check path structure.")
        return

    # ====================================================
    # ğŸš¨ [ìˆ˜ì •] Real/Fake ë°¸ëŸ°ì‹± ë¡œì§ ì¶”ê°€
    # ====================================================
    real_videos = []
    fake_videos = []

    for v in all_videos:
        path_str = str(v).lower()
        # Real í´ë”ëª… í‚¤ì›Œë“œ í™•ì¸ (FF++: original, Celeb-DF: celeb-real, youtube-real)
        if (
            "original" in path_str
            or "celeb-real" in path_str
            or "youtube-real" in path_str
        ):
            real_videos.append(v)
        else:
            fake_videos.append(v)

    print(
        f"[INFO] Found total: {len(all_videos)} (Real: {len(real_videos)}, Fake: {len(fake_videos)})"
    )

    # ì œí•œ ì ìš© (ê°ê° ì ˆë°˜ì”© ê°€ì ¸ì˜¤ê¸°)
    final_videos = []
    if VIDEO_LIMIT_PER_DATASET > 0:
        half_limit = VIDEO_LIMIT_PER_DATASET // 2

        # ì„ì–´ì„œ ëœë¤í•˜ê²Œ ë½‘ê¸° ìœ„í•´ shuffle
        random.shuffle(real_videos)
        random.shuffle(fake_videos)

        # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ìˆëŠ” ë§Œí¼ë§Œ ê°€ì ¸ì˜¤ê¸°
        selected_reals = real_videos[:half_limit]
        selected_fakes = fake_videos[:half_limit]

        final_videos = selected_reals + selected_fakes
        print(
            f"[WARN] Balanced Limiting: Used {len(selected_reals)} Real + {len(selected_fakes)} Fake videos."
        )
    else:
        final_videos = all_videos
    # ====================================================

    with INDEX_CSV.open("w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["video_path", "frame_path"])

        for v in tqdm(final_videos, desc=f"Extracting {dataset['name']}"):
            saved_frames = extract_frames_from_video(v, FRAMES_DIR)
            for fp in saved_frames:
                rel_video_path = v.relative_to(PROJECT_ROOT)
                rel_frame_path = fp.relative_to(PROJECT_ROOT)
                writer.writerow([str(rel_video_path), str(rel_frame_path)])

    print(f"[DONE] frames saved to: {FRAMES_DIR.name}")
    print(f"[DONE] index csv: {INDEX_CSV.name}")


# ============================================
# III. ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ============================================


def main():
    print("\n[MASTER PROCESS] Starting Dual-Dataset Preprocessing Pipeline...")

    for dataset in DATASETS:
        print("\n" + "=" * 60)
        print(f"ğŸš€ Processing Dataset: {dataset['name']}")
        print("=" * 60)

        # ---------------------------------------------
        # 1. í”„ë ˆì„ ì¶”ì¶œ (Frame Extraction)
        # ---------------------------------------------
        print("\n[STEP 1/3] Starting Frame Extraction (Balanced)...")
        run_extraction_pipeline(dataset)

        # ---------------------------------------------
        # 2. ì–¼êµ´ í¬ë¡­ ë° ì •ë ¬ (Face Cropping)
        # ---------------------------------------------
        print("\n[STEP 2/3] Starting Face Cropping...")
        try:
            run_face_extraction_runner(
                dataset["frame_csv"], dataset["face_output_dir"], dataset["face_csv"]
            )
            print(f"[DONE] Step 2 Complete.")
        except Exception as e:
            print(f"[FATAL ERROR] Step 2 failed: {e}")
            continue

        # ---------------------------------------------
        # 3. ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„± (Indexing & Split)
        # ---------------------------------------------
        print("\n[STEP 3/3] Starting Train/Val/Test Indexing...")
        try:
            run_indexing_runner(dataset["face_csv"], dataset["index_prefix"])
            print(f"[DONE] Step 3 Complete.")
        except Exception as e:
            print(f"[FATAL ERROR] Step 3 failed: {e}")
            continue

    print("\n\nğŸ‰ [MASTER PROCESS] ALL DATA PREPARATION COMPLETE!")


if __name__ == "__main__":
    main()
